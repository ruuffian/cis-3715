February 14th, 2023

Summary of What I think I missed:
- Build Model
	- $f(x_i) = w_0 + w_1x_{i,1} + w_2x_{i,2} \dots w_dx_{i, d}$ 
- Split the data set, about 70/30 training and testing.
- Gradient Descent
	- $x_{t+1} = x_t - n \nabla f(x_t)$ 
	- Basically move in the opposite direction of the gradient to minimize the loss function ($f(x_t)$).